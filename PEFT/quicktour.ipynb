{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c420183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 设置镜像\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "784f9db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先试用模型 - 简单的文本生成测试\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# 设置缓存目录\n",
    "cache_dir = \"../models_cache\"  # 您可以修改为任意您想要的路径\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# 使用指定的缓存目录下载模型\n",
    "# TODO 换小点的huggingface模型测试\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"bigscience/mt0-small\",\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "# 加载对应的tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/mt0-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fb9bcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入文本: translate English to chinese: Hello, how are you ?\n",
      "Token IDs: tensor([[ 37194,   5413,    288,    259, 116983,    267,  30273,    261,   2606,\n",
      "            418,    521,    259,    291,      1]])\n",
      "输入长度: torch.Size([1, 14])\n",
      "生成结果: Bonjour,你呢?\n",
      "生成结果: Bonjour,你呢?\n"
     ]
    }
   ],
   "source": [
    "# 测试输入\n",
    "test_text = \"translate English to chinese: Hello, how are you ?\"\n",
    "print(f\"输入文本: {test_text}\")\n",
    "\n",
    "# 编码输入\n",
    "inputs = tokenizer(test_text, return_tensors=\"pt\")\n",
    "print(f\"Token IDs: {inputs['input_ids']}\")\n",
    "print(f\"输入长度: {inputs['input_ids'].shape}\")\n",
    "\n",
    "# 生成输出（简单测试）\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'], \n",
    "        max_length=50, \n",
    "        num_beams=2, \n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "# 解码输出\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"生成结果: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9645a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入文本: 翻译成英文：你好，你是谁？\n",
      "Token IDs: tensor([[   259, 138961,   4449, 102031,    267,   4235,   3586,    261,   4235,\n",
      "         156695,    291,      1]])\n",
      "输入长度: torch.Size([1, 12])\n",
      "生成结果: 你好,你好?\n",
      "生成结果: 你好,你好?\n"
     ]
    }
   ],
   "source": [
    "# 测试输入\n",
    "test_text = \"翻译成英文：你好，你是谁？\"\n",
    "print(f\"输入文本: {test_text}\")\n",
    "\n",
    "# 编码输入\n",
    "inputs = tokenizer(test_text, return_tensors=\"pt\")\n",
    "print(f\"Token IDs: {inputs['input_ids']}\")\n",
    "print(f\"输入长度: {inputs['input_ids'].shape}\")\n",
    "\n",
    "# 生成输出（简单测试）\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'], \n",
    "        max_length=50, \n",
    "        num_beams=2, \n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "# 解码输出\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"生成结果: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e21dcdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a55dd801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 344,064 || all params: 300,520,832 || trainable%: 0.1145\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceb8c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../train_cache/bigscience/mt0-large-lora\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f3dfae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载 BelleGroup/train_2M_CN 数据集\n",
      "数据集大小: 1000\n",
      "数据集示例:\n",
      "示例 1:\n",
      "  指令: 对以下代码进行调试。\n",
      "def add(a,b):\n",
      "  return a + b\n",
      "  print(add(2,3))\n",
      "  输入: \n",
      "  输出: 这段代码的问题是在函数内部， `print` 语句出现在 `return` 语句之后，也就是说函数在 `return` 语句处结束，因此 `print` 语句永远不会被执行。\n",
      "要解决这个问题，只需在 `return` 语句之前移到 `print` 语句。修改后的代码如下所示：\n",
      "```\n",
      "def add(a, b):\n",
      "    print(a + b)\n",
      "    return a + b\n",
      "add(2, 3)\n",
      "```\n",
      "这样就可以在调用 `add` 函数时打印出结果，并在函数内部返回 `a + b` 值，使其在需要时可以接收和使用。同时，注意在调用 `add` 函数时，不需要再使用 `print` 方法了。\n",
      "\n",
      "示例 2:\n",
      "  指令: 为一个新的智能手机应用程序起个优秀的名称并写一句简短的介绍。\n",
      "\n",
      "  输入: \n",
      "  输出: 为一个新的智能手机应用程序起一个优秀的名称可以考虑以下几点：\n",
      "1. 名称应该简单易记，不宜太长；\n",
      "2. 名称应该与应用程序的主要功能或特点相关；\n",
      "3. 名称应该引起人们的兴趣和好奇心。\n",
      "举个例子，如果我们的应用程序是一款健康管理应用程序，那么可以考虑以下几个名称：\n",
      "1. HealthBoss\n",
      "2. FitMeUp\n",
      "3. HealthMate\n",
      "4. BodyBest\n",
      "5. HealthLab\n",
      "其中，HealthBoss强调应用程序可帮助用户成为健康的老板，FitMeUp强调应用程序可帮助用户达到健康生活的目标，HealthMate强调应用程序是用户健康的伙伴，BodyBest强调应用程序可通过健康管理帮助用户获得最佳身体状态，HealthLab强调应用程序可帮助用户进行健康诊断和实验。\n",
      "一句简短的介绍可包含以下几个要素：\n",
      "1. 应用程序的名称；\n",
      "2. 应用程序的主要功能或特点；\n",
      "3. 应用程序的受众或目标用户。\n",
      "例如，以上面的例子为基础，可以写出以下几个简短的介绍：\n",
      "1. HealthBoss – 健康管理，你的健康老板。\n",
      "2. FitMeUp – 达成健康目标，轻松自在，适合每个人。\n",
      "3. HealthMate – 与您在一起的健康伙伴。\n",
      "4. BodyBest – 拥有最好的身体，成为最好的自己。\n",
      "5. HealthLab – 健康管理和诊断，为您量身定制。\n",
      "\n",
      "示例 3:\n",
      "  指令: 在给定的代码示例中，修复一个特定的错误。\n",
      "以下是一个Python代码示例：\n",
      "def sum(a, b):\n",
      "  return a + b\n",
      "print(sum(1, 2, 3))\n",
      "代码中的错误是什么？\n",
      "  输入: \n",
      "  输出: 该Python代码中的错误是调用sum()函数时传递了3个参数，而该函数只接受2个参数。因此，代码会引发TypeError异常。\n",
      "要修复这个错误，只需从函数调用中删除多余的参数即可。下面是代码示例的修复版本：\n",
      "```\n",
      "def sum(a, b):\n",
      "  return a + b\n",
      "print(sum(1, 2))\n",
      "```\n",
      "在修复后的代码版本中，调用sum()函数时只传递了两个参数，因此将不会报告TypeError异常。这将打印出3，即1加2的结果。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载中文指令数据集\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# 方案1: 使用BelleGroup/train_2M_CN (中文指令数据集)\n",
    "# 这是一个高质量的中文指令微调数据集\n",
    "try:\n",
    "    # 加载数据集的一个子集用于演示\n",
    "    dataset = load_dataset(\"BelleGroup/train_2M_CN\", split=\"train[:1000]\", cache_dir=cache_dir)\n",
    "    print(\"成功加载 BelleGroup/train_2M_CN 数据集\")\n",
    "except:\n",
    "    print(\"无法加载 BelleGroup/train_2M_CN，尝试其他数据集...\")\n",
    "    \n",
    "    # 方案2: 使用tatsu-lab/alpaca_chinese (中文Alpaca数据集)\n",
    "    try:\n",
    "        dataset = load_dataset(\"tatsu-lab/alpaca_chinese\", split=\"train[:1000]\", cache_dir=cache_dir)\n",
    "        print(\"成功加载 tatsu-lab/alpaca_chinese 数据集\")\n",
    "    except:\n",
    "        print(\"无法加载 tatsu-lab/alpaca_chinese，创建示例数据集...\")\n",
    "        \n",
    "        # 方案3: 创建示例中文指令数据集\n",
    "        sample_data = [\n",
    "            {\n",
    "                \"instruction\": \"将下面的句子翻译成英文\",\n",
    "                \"input\": \"今天天气很好。\",\n",
    "                \"output\": \"The weather is very nice today.\"\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"回答下面的问题\",\n",
    "                \"input\": \"什么是人工智能？\",\n",
    "                \"output\": \"人工智能（AI）是指让机器模拟人类智能的技术，包括学习、推理、感知等能力。\"\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"总结下面的文本\",\n",
    "                \"input\": \"机器学习是人工智能的一个重要分支，它使计算机能够在不被明确编程的情况下学习。\",\n",
    "                \"output\": \"机器学习是AI的分支，让计算机自动学习而无需明确编程。\"\n",
    "            }\n",
    "        ] * 100  # 重复数据以创建更大的数据集\n",
    "        \n",
    "        dataset = Dataset.from_list(sample_data)\n",
    "        print(\"创建了示例中文指令数据集\")\n",
    "\n",
    "print(f\"数据集大小: {len(dataset)}\")\n",
    "print(\"数据集示例:\")\n",
    "for i in range(min(3, len(dataset))):\n",
    "    print(f\"示例 {i+1}:\")\n",
    "    print(f\"  指令: {dataset[i]['instruction']}\")\n",
    "    print(f\"  输入: {dataset[i]['input']}\")\n",
    "    print(f\"  输出: {dataset[i]['output']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c287efa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始数据预处理...\n",
      "训练集大小: 900\n",
      "测试集大小: 100\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理函数\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    将指令数据转换为模型输入格式\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for instruction, input_text, output in zip(examples[\"instruction\"], examples[\"input\"], examples[\"output\"]):\n",
    "        # 构建输入文本：指令 + 输入\n",
    "        if input_text and input_text.strip():\n",
    "            input_text_formatted = f\"指令: {instruction}\\n输入: {input_text}\\n输出: \"\n",
    "        else:\n",
    "            input_text_formatted = f\"指令: {instruction}\\n输出: \"\n",
    "        \n",
    "        inputs.append(input_text_formatted)\n",
    "        targets.append(output)\n",
    "    \n",
    "    # Tokenize输入和输出\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=True)\n",
    "    labels = tokenizer(targets, max_length=512, truncation=True, padding=True)\n",
    "    \n",
    "    # 设置labels\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# 应用预处理\n",
    "print(\"开始数据预处理...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# 分割数据集\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "tokenized_datasets = {\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"test\": train_test_split[\"test\"]\n",
    "}\n",
    "\n",
    "print(f\"训练集大小: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"测试集大小: {len(tokenized_datasets['test'])}\")\n",
    "\n",
    "# 数据收集器\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71eb0404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评估指标函数已定义\n"
     ]
    }
   ],
   "source": [
    "# 评估指标\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "def computeMetrics(evalPreds):\n",
    "    \"\"\"\n",
    "    计算评估指标\n",
    "    \"\"\"\n",
    "    predictions, labels = evalPreds\n",
    "    \n",
    "    try:\n",
    "        # 解码预测结果和标签\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "        \n",
    "        # 将-100替换为tokenizer.pad_token_id用于解码\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "        \n",
    "        # 解码\n",
    "        decodedPreds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        decodedLabels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # 计算BLEU分数（如果可用）\n",
    "        bleuMetric = evaluate.load(\"bleu\")\n",
    "        bleuScore = bleuMetric.compute(\n",
    "            predictions=decodedPreds, \n",
    "            references=[[label] for label in decodedLabels]\n",
    "        )\n",
    "        \n",
    "        # 计算平均长度作为辅助指标\n",
    "        predLens = [len(pred.split()) for pred in decodedPreds]\n",
    "        labelLens = [len(label.split()) for label in decodedLabels]\n",
    "        \n",
    "        result = {\n",
    "            \"bleu\": bleuScore[\"bleu\"],\n",
    "            \"avg_pred_length\": np.mean(predLens),\n",
    "            \"avg_label_length\": np.mean(labelLens),\n",
    "        }\n",
    "        \n",
    "        # 显示一些样例\n",
    "        print(\"\\n=== 评估样例 ===\")\n",
    "        for i in range(min(3, len(decodedPreds))):\n",
    "            print(f\"预测 {i+1}: {decodedPreds[i]}\")\n",
    "            print(f\"标签 {i+1}: {decodedLabels[i]}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"评估过程中出现错误: {e}\")\n",
    "        # 返回基本指标\n",
    "        predLens = [len(str(pred).split()) for pred in predictions.flatten()]\n",
    "        labelLens = [len(str(label).split()) for label in labels.flatten()]\n",
    "        \n",
    "        return {\n",
    "            \"avg_pred_length\": np.mean(predLens),\n",
    "            \"avg_label_length\": np.mean(labelLens),\n",
    "        }\n",
    "\n",
    "print(\"评估指标函数已定义\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5d10a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='901' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 901/1800 02:24 < 02:24, 6.21 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 11/100 00:14 < 02:11, 0.68 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.72 GiB. GPU 0 has a total capacity of 16.00 GiB of which 0 bytes is free. Of the allocated memory 20.41 GiB is allocated by PyTorch, and 4.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[0;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      5\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcomputeMetrics,\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\conda_env\\hf_course\\lib\\site-packages\\transformers\\trainer.py:2237\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2235\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2238\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\conda_env\\hf_course\\lib\\site-packages\\transformers\\trainer.py:2694\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2691\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2694\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\n\u001b[0;32m   2696\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2700\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32md:\\conda_env\\hf_course\\lib\\site-packages\\transformers\\trainer.py:3133\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[0;32m   3131\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 3133\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3134\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m   3136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[1;32md:\\conda_env\\hf_course\\lib\\site-packages\\transformers\\trainer.py:3082\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[1;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[0;32m   3081\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 3082\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3083\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   3085\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32md:\\conda_env\\hf_course\\lib\\site-packages\\transformers\\trainer.py:4249\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4246\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   4248\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 4249\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4250\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   4253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   4254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4257\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4259\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32md:\\conda_env\\hf_course\\lib\\site-packages\\transformers\\trainer.py:4471\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4469\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function(logits)\n\u001b[0;32m   4470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_eval_metrics \u001b[38;5;129;01mor\u001b[39;00m description \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 4471\u001b[0m         \u001b[43mall_preds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4473\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function(labels)\n",
      "File \u001b[1;32md:\\conda_env\\hf_course\\lib\\site-packages\\transformers\\trainer_pt_utils.py:317\u001b[0m, in \u001b[0;36mEvalLoopContainer.add\u001b[1;34m(self, tensors)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat \u001b[38;5;28;01melse\u001b[39;00m [tensors]\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat:\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mappend(tensors)\n",
      "File \u001b[1;32md:\\conda_env\\hf_course\\lib\\site-packages\\transformers\\trainer_pt_utils.py:129\u001b[0m, in \u001b[0;36mnested_concat\u001b[1;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(new_tensors), (\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m     )\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[1;32md:\\conda_env\\hf_course\\lib\\site-packages\\transformers\\trainer_pt_utils.py:129\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(new_tensors), (\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m     )\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[1;32md:\\conda_env\\hf_course\\lib\\site-packages\\transformers\\trainer_pt_utils.py:131\u001b[0m, in \u001b[0;36mnested_concat\u001b[1;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[0;32m    134\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    135\u001b[0m     )\n",
      "File \u001b[1;32md:\\conda_env\\hf_course\\lib\\site-packages\\transformers\\trainer_pt_utils.py:89\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[1;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[0;32m     86\u001b[0m tensor2 \u001b[38;5;241m=\u001b[39m atleast_1d(tensor2)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Let's figure out the new shape\u001b[39;00m\n\u001b[0;32m     92\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.72 GiB. GPU 0 has a total capacity of 16.00 GiB of which 0 bytes is free. Of the allocated memory 20.41 GiB is allocated by PyTorch, and 4.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=computeMetrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dabd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../output_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25daafc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
